{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取ISMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 提取中国732个站点多层土壤水分数据集\n",
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# # 定义文件夹路径\n",
    "# folder_path = 'C:/Users/pc/Desktop/嗑盐/Y_data/中国732个站点多层土壤水分数据集/stn_Mon_sm_vol'\n",
    "\n",
    "# # 获取文件夹下的所有txt文件\n",
    "# txt_files = [file for file in os.listdir(folder_path) if file.endswith('.txt')]\n",
    "\n",
    "# # 创建一个空的DataFrame来存储数据\n",
    "# data = pd.DataFrame()\n",
    "\n",
    "# # 遍历每个txt文件\n",
    "# for txt_file in txt_files:\n",
    "#     # 提取站号\n",
    "#     station_id = txt_file.split('.')[0]\n",
    "    \n",
    "#     # 读取txt文件为DataFrame\n",
    "#     file_path = os.path.join(folder_path, txt_file)\n",
    "#     df = pd.read_csv(file_path, sep=' ', header=None, names=['station', 'year', 'month', 'soil_moisture','1','2','3','4','5'])\n",
    "#     df = df.iloc[:,:-5]\n",
    "#     # 将站号作为列名，土壤湿度值作为对应列的值\n",
    "#     df.set_index(['year', 'month'], inplace=True)\n",
    "#     df.drop(columns=['station'], inplace=True)\n",
    "#     df.columns = [station_id]\n",
    "    \n",
    "#     # 合并到总的数据DataFrame\n",
    "#     data = pd.concat([data, df], axis=1)\n",
    "\n",
    "# # 保存数据到CSV文件\n",
    "# output_csv_file = 'C:/Users/pc/Desktop/嗑盐/Y_data/中国732个站点多层土壤水分数据集/merged_data.csv'\n",
    "# data.to_csv(output_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. ismn data in newsites\n",
    "\n",
    "# import datetime\n",
    "\n",
    "# from ismn.interface import ISMN_Interface\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import math\n",
    "# import os\n",
    "# import geopandas as gpd\n",
    "# from shapely.geometry import Point\n",
    "# import shutil\n",
    "# import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #data_path = r\"/share/yangjunran/Data_separate_files_19500101_20220407_9419_21BN_20220407/\"\n",
    "# data_path = r\"D:/dataset/Data_separate_files_19500101_20220407_9419_21BN_20220407/\"\n",
    "# #data_path = r\"D:/dataset/my_test/\"\n",
    "# ismn_data = ISMN_Interface(data_path, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. smap vs. newsites_data cc\n",
    "# # smap(\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\smap_newsites.csv\")\n",
    "# # newsites_data(\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\newsites_data.csv\")\n",
    "\n",
    "# # 2.1 convert xlsx to csv\n",
    "# import pandas as pd\n",
    "\n",
    "# # 读取XLSX文件\n",
    "# xlsx_file_path = 'C:\\\\Users\\\\pc\\\\Desktop\\\\嗑盐\\\\Y_data\\\\站点位置及田持信息.xlsx'  # 输入XLSX文件路径\n",
    "# data_frame = pd.read_excel(xlsx_file_path)\n",
    "\n",
    "# # 转换为CSV并保存\n",
    "# csv_file_path = '..\\\\Y_data\\\\站点位置及田持信息.csv'  # 输出CSV文件路径\n",
    "# data_frame.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# # 2.2 merge newsites_name to the csv\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# new_sites = pd.read_csv('..\\\\Y_data\\\\站点位置及田持信息.csv')\n",
    "# new_sm_data = pd.read_csv('..\\\\Y_data\\\\土壤重量含水率监测信息.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #原始数据---> Y格式\n",
    "# #已知有一csv文件，各列分别为：站号、时间、一站多点标识、10厘米深度重量含水率、20厘米深度重量含水率、30厘米深度重量含水率、40厘米深度重量含水率，\n",
    "# #给出python代码，生成一个新的csv文件，索引为时间，各列分别为每个站号，对应的值为10厘米深度重量含水率\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # 读取原始CSV文件\n",
    "# input_csv = r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\土壤重量含水率监测信息.csv\"  # 输入CSV文件路径\n",
    "# data_frame = pd.read_csv(input_csv)\n",
    "\n",
    "# # 选择所需的列\n",
    "# selected_columns = ['时间', '站号', '10厘米深度重量含水率']\n",
    "# filtered_data = data_frame[selected_columns]\n",
    "\n",
    "# # 使用pivot_table将数据重塑为透视表，索引为时间，列为站号\n",
    "# pivot_data = pd.pivot_table(filtered_data, values='10厘米深度重量含水率', index='时间', columns='站号')\n",
    "\n",
    "# # 将数据保存为新的CSV文件\n",
    "# output_csv = 'output.csv'  # 输出CSV文件路径\n",
    "# pivot_data.to_csv(output_csv)\n",
    "\n",
    "# print(\"新的CSV文件已生成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Y格式（站号）---> Y格式（站名）\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # 读取a.csv文件和b.csv文件\n",
    "# a_csv = r\"C:\\Users\\pc\\Desktop\\嗑盐\\code\\output.csv\"  # a.csv文件路径\n",
    "# b_csv = r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\站点位置及田持信息.csv\"  # b.csv文件路径\n",
    "\n",
    "# a_data = pd.read_csv(a_csv, index_col='时间')\n",
    "# b_data = pd.read_csv(b_csv)\n",
    "\n",
    "# # 获取站号和站名的映射关系\n",
    "# station_mapping = dict(zip(b_data['站号'], b_data['站名']))\n",
    "\n",
    "# # 使用映射关系修改a.csv文件的列名\n",
    "# a_data.columns = [station_mapping.get(col, col) for col in a_data.columns]\n",
    "\n",
    "# # 保存修改后的a.csv文件\n",
    "# modified_a_csv = 'modified_a.csv'  # 输出修改后的a.csv文件路径\n",
    "# a_data.to_csv(modified_a_csv)\n",
    "\n",
    "# print(\"修改后的a.csv文件已保存！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 提取给定经纬度（站点）的smap数据\n",
    "\n",
    "# import os\n",
    "# import h5py\n",
    "# import glob\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from datetime import datetime\n",
    "# from tqdm import tqdm\n",
    "# from ease_grid import EASE2_grid\n",
    "\n",
    "# grids = EASE2_grid(9008.05, proj='G')\n",
    "\n",
    "\n",
    "\n",
    "# # 从文件名中提取日期\n",
    "# def extract_date(file_name):\n",
    "#     return datetime.strptime(file_name.split(\"_\")[5], \"%Y%m%d\")\n",
    "\n",
    "# # 处理单个文件\n",
    "# def process_file(file_path):\n",
    "#     data = h5py.File(file_path, 'r')\n",
    "#     sm = data['soil_moisture']\n",
    "#     # lat_coords = data['latitude']\n",
    "#     # lon_coords = data['longitude']\n",
    "#     one_values = []\n",
    "#     point_name_list = []\n",
    "#     t1 = pd.read_csv(r'F:\\project\\To_土壤墒情监测_人工站\\站点位置及田持信息.csv')\n",
    "#     for i,col in t1.iterrows():\n",
    "#         # intro = pd.read_csv(i, index_col=0)\n",
    "#         input_lon = col['经度']\n",
    "#         input_lat = col['纬度']\n",
    "#         lat_ind = np.argmin(abs(grids.latdim - input_lat))\n",
    "#         lon_ind = np.argmin(abs(grids.londim - input_lon))\n",
    "#         one_values.append(sm[lat_ind, lon_ind])\n",
    "#         point_name_list.append(col['站名'])\n",
    "#     return one_values,point_name_list\n",
    "\n",
    "\n",
    "# # 过滤出.h5文件\n",
    "# files = glob.glob(r'D:\\dataset\\smap\\*.h5')\n",
    "\n",
    "# # 按日期排序文件\n",
    "# files.sort(key=extract_date)\n",
    "\n",
    "# # 处理每个文件并收集结果\n",
    "# all_values = []\n",
    "\n",
    "# for file in tqdm(files):\n",
    "#     one_value,point_name_list = process_file(file)\n",
    "#     all_values.append(one_value)\n",
    "\n",
    "# # 将结果转换为pandas DataFrame\n",
    "# all_values = np.array(all_values).T\n",
    "# result = pd.DataFrame(all_values, columns=[extract_date(file) for file in files])\n",
    "\n",
    "# # 将结果转置成Y_data格式\n",
    "# result1 = result.T\n",
    "# result1.columns = point_name_list\n",
    "# result1[result1 == -9999.0] = np.nan\n",
    "\n",
    "# # 将结果保存到CSV文件\n",
    "# result1.to_csv(\"smap_yellow_new_newsites.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #正确的\n",
    "# #根据站点经纬度提取era5数据\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import xarray as xr\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # 定义站点数据文件路径\n",
    "# station_csv_file = 'C:/Users/pc/Desktop/嗑盐/Y_data/new_new_sites_yellow_points.csv'\n",
    "\n",
    "# # 定义nc文件夹路径\n",
    "# nc_folder = 'D:/dataset/ERA5_yellow_800/'\n",
    "\n",
    "# # 从CSV文件读取站点数据\n",
    "# station_data = pd.read_csv(station_csv_file)\n",
    "\n",
    "# # 创建一个空的DataFrame来存储提取后的数据，行索引为时间，列名为站点名\n",
    "# extracted_data = pd.DataFrame()\n",
    "\n",
    "# # 遍历每个站点\n",
    "# for station_row in tqdm(station_data.itertuples(), total=len(station_data), desc='Processing Stations'):\n",
    "#     station_lon = station_row.lon\n",
    "#     station_lat = station_row.lat\n",
    "#     station_name = station_row.Station_ID\n",
    "#     mid_data = pd.DataFrame()\n",
    "#     # 遍历每个nc文件 即每个年份\n",
    "#     for nc_file in os.listdir(nc_folder):\n",
    "#         if nc_file.endswith('.nc'):\n",
    "#             # 从文件名中提取年份\n",
    "#             year = int(nc_file.split('_')[0])\n",
    "\n",
    "#             # 使用xarray库读取nc文件\n",
    "#             ds = xr.open_dataset(os.path.join(nc_folder, nc_file))\n",
    "\n",
    "#             # 提取longitude、latitude、time和swvl1变量\n",
    "#             longitude = ds['longitude'].values\n",
    "#             latitude = ds['latitude'].values\n",
    "#             time = ds['time'].values\n",
    "#             swvl1 = ds['swvl1'].values\n",
    "\n",
    "#             # 找到最近的nc坐标索引\n",
    "#             lon_ind = (abs(longitude - station_lon)).argmin()\n",
    "#             lat_ind = (abs(latitude - station_lat)).argmin()\n",
    "\n",
    "#             # 获取对应的swvl1值\n",
    "#             swvl1_values = swvl1[:, lat_ind, lon_ind]\n",
    "\n",
    "#             # 创建一个以时间为索引的DataFrame\n",
    "#             # 一次取一个站点的一年数据【即一小列数据】\n",
    "#             df = pd.DataFrame(swvl1_values, index=pd.to_datetime(time, format='%Y-%m-%dT%H:%M:%S'), columns=[station_name])\n",
    "\n",
    "#             # 将当前站点的数据合并到提取后的数据中\n",
    "#             if mid_data.empty:\n",
    "#                 mid_data = df\n",
    "#             else:\n",
    "#                 mid_data = pd.concat([mid_data, df], axis=0, ignore_index=False)\n",
    "\n",
    "#             # 关闭xarray数据集\n",
    "#             ds.close()\n",
    "#     if extracted_data.empty:\n",
    "#         extracted_data = mid_data\n",
    "#     else:\n",
    "#         extracted_data = pd.concat([extracted_data, mid_data], axis=1)\n",
    "# # 保存提取后的数据到CSV文件\n",
    "# #extracted_data.to_csv('era5_yellow_new_newsites.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 根据站点经纬度提取ESACCI数据\n",
    "# # 重构一下逻辑，哪些是高io操作，放在外面\n",
    "# # position=0, desc=\"each year\", leave=False, colour='green', ncols=80\n",
    "# # position=1, desc=\"each items\", leave=False, colour='green', ncols=80\n",
    "# # 逻辑：首先源数据是每个年份子文件夹，每个子文件夹里面是每天的nc文件\n",
    "# # 那么第一个for循环就是遍历每个年份[1987-2021]，第二个for循环就是遍历每个子文件夹[每年的365/6天]，第三个for循环就是遍历每个站点[170+个站点]\n",
    "# # concat操作非常重要也容易出错，本次逻辑是先纵向拼接每个站点每天的数据【第三个for】，再横向拼接所有站点每天的一行数据【第二个for】\n",
    "\n",
    "# #正确的\n",
    "# #根据站点经纬度提取ESACCI数据\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import xarray as xr\n",
    "# from tqdm import tqdm\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# # 定义站点数据文件路径\n",
    "# station_csv_file = 'C:/Users/pc/Desktop/嗑盐/Y_data/new_new_sites_yellow_points.csv'\n",
    "\n",
    "# # 定义nc文件夹路径\n",
    "# nc_folder = r'D:\\dataset\\ESACCI'\n",
    "\n",
    "\n",
    "\n",
    "# # 从CSV文件读取站点数据\n",
    "# station_data = pd.read_csv(station_csv_file)\n",
    "\n",
    "\n",
    "# # 遍历每个nc文件 即每个年份\n",
    "# # final_data 存储每一年全部站点的数据\n",
    "# # 最终想要的当然要放在循环外面\n",
    "# #final_data = pd.DataFrame()\n",
    "# # for year_file in tqdm(os.listdir(nc_folder), total=len(os.listdir(nc_folder)), desc='Processing each year'):\n",
    "# final_data = pd.DataFrame()\n",
    "# for year_file in tqdm(os.listdir(nc_folder), position=0, desc=\"each year\", leave=False, colour='green', ncols=80):\n",
    "#     for nc_file in tqdm(os.listdir(os.path.join(nc_folder,year_file)), position=0, desc=\"each items\", leave=False, colour='green', ncols=80):\n",
    "#         # 从文件名中提取年份\n",
    "#         year = int(nc_file.split('-')[-2][:4])\n",
    "\n",
    "#         # 使用xarray库读取nc文件\n",
    "#         ds = xr.open_dataset(os.path.join(nc_folder, year_file, nc_file))\n",
    "\n",
    "#         # 提取longitude、latitude、time和swvl1变量\n",
    "#         longitude = ds['lon'].values\n",
    "#         latitude = ds['lat'].values\n",
    "#         time = ds['time'].values\n",
    "\n",
    "#         swvl1 = ds['sm'].values\n",
    "        \n",
    "#         # 关闭xarray数据集\n",
    "#         ds.close()\n",
    "\n",
    "#         df = pd.DataFrame()\n",
    "#         # oneday_allsites_data 存储每一天全部站点的数据【即每一小行数据】\n",
    "#         oneday_allsites_data = pd.DataFrame()\n",
    "#         # for station_row in tqdm(station_data.itertuples(), total=len(station_data), desc='Processing Stations'):\n",
    "#         for station_row in station_data.itertuples():\n",
    "#             station_lon = station_row.lon\n",
    "#             station_lat = station_row.lat\n",
    "#             station_name = station_row.Station_ID\n",
    "#             # 找到最近的nc坐标索引\n",
    "#             lon_ind = (abs(longitude - station_lon)).argmin()\n",
    "#             lat_ind = (abs(latitude - station_lat)).argmin()\n",
    "\n",
    "#             # 获取对应的swvl1值\n",
    "#             swvl1_values = swvl1[:, lat_ind, lon_ind]\n",
    "\n",
    "#             # 创建一个以时间为索引的DataFrame\n",
    "#             # 一次取一个站点的一天数据【即一小列数据】\n",
    "#             df = pd.DataFrame(swvl1_values, index=pd.to_datetime(time, format='%Y-%m-%dT%H:%M:%S'), columns=[station_name])\n",
    "\n",
    "#             # 将当前站点的数据合并到提取后的数据中\n",
    "#             # for nc_file循环后（1年内的全部nc），mid_data是一个站点的一年数据\n",
    "#             if oneday_allsites_data.empty:\n",
    "#                 oneday_allsites_data = df\n",
    "#             else:\n",
    "#                 # 左右拼接[就是一行(一天)]\n",
    "#                 oneday_allsites_data = pd.concat([oneday_allsites_data, df], axis=1)\n",
    "            \n",
    "#             #print(oneday_allsites_data)\n",
    "#         if final_data.empty:\n",
    "#             final_data = oneday_allsites_data\n",
    "#         else:\n",
    "#             # 上下拼接\n",
    "#             final_data = pd.concat([final_data, oneday_allsites_data], axis=0, ignore_index=False)\n",
    "#         # # 关闭xarray数据集\n",
    "#         # ds.close()\n",
    "# # 保存提取后的数据到CSV文件\n",
    "# final_data.to_csv('ESACCI_yellow_new_newsites.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 未使用的代码\n",
    "# #计算  smap_newsites vs. situ_newsites cc相关系数\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # 读取ismn_yellow_result.csv文件\n",
    "# ismn_data = pd.read_csv(\"C:/Users/pc/Desktop/嗑盐/Y_data/ismn_yellow_result.csv\", index_col=0)\n",
    "\n",
    "# # 读取smap_ismn.csv文件\n",
    "# smap_data = pd.read_csv(\"C:/Users/pc/Desktop/嗑盐/Y_data/smap_ismn.csv\", index_col=0)\n",
    "\n",
    "# # 获取站点名称列表\n",
    "# site_names = ismn_data.columns.tolist()\n",
    "\n",
    "# # 遍历每个站点\n",
    "# for site_name in site_names:\n",
    "#     # 获取ismn_yellow_result.csv文件中站点的数据列\n",
    "#     ismn_column = ismn_data[site_name]\n",
    "\n",
    "#     # 获取smap_ismn.csv文件中站点的数据列\n",
    "#     smap_column = smap_data[site_name]\n",
    "\n",
    "#     # 去除包含缺失值的行\n",
    "#     combined_data = pd.concat([ismn_column, smap_column], axis=1).dropna()\n",
    "\n",
    "#     # 计算相关系数\n",
    "#     correlation = combined_data.corr().iloc[0, 1]\n",
    "\n",
    "#     print(f\"站点：{site_name}，相关系数：{correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #计算  smap_newsites vs. era5/situ_newsites/situ_ismn cc相关系数\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # 读取csv文件\n",
    "# # df1 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\newsites_yellow_result.csv\", index_col=0)\n",
    "# # df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\smap_newsites.csv\", index_col=0)\n",
    "# # df1 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\ismn_yellow_result.csv\", index_col=0)\n",
    "# # df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\smap_ismnsites.csv\", index_col=0)\n",
    "# # df1 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\ismn_yellow_result.csv\", index_col=0)\n",
    "# # df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\smap_ismnsites.csv\", index_col=0)\n",
    "# df1 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\newsites_yellow_result.csv\", index_col=0)\n",
    "# df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\era5_newsites.csv\", index_col=0)\n",
    "\n",
    "# # 将索引转换为datetime类型\n",
    "# df1.index = pd.to_datetime(df1.index, format='mixed')\n",
    "\n",
    "# # 舍去时间戳的时刻部分，只保留日期\n",
    "# df1.index = df1.index.date\n",
    "\n",
    "# # 你也可以将索引再转换为datetime类型\n",
    "# df1.index = pd.to_datetime(df1.index)\n",
    "# #print(df1.index)\n",
    "\n",
    "# # df1.index = pd.to_datetime(df1.index, format='%Y/%m/%d')\n",
    "# df1.index = pd.to_datetime(df1.index, format='mixed')\n",
    "# df2.index = pd.to_datetime(df2.index, format='mixed')\n",
    "\n",
    "# # 保证取值在相同时间索引下\n",
    "# df1 = df1[df1.index.isin(df2.index)]\n",
    "# df2 = df2[df2.index.isin(df1.index)]\n",
    "\n",
    "# # 打印每个时间索引下的每个站点的数值\n",
    "# for time_index in df1.index:\n",
    "#     #print(f\"时间索引：{time_index}\")\n",
    "#     for column in df1.columns:\n",
    "#         if column in df2.columns:\n",
    "#             #print(f\"站点{column}在文件1中的数值为: {df1.at[time_index, column]}，在文件2中的数值为: {df2.at[time_index, column]}\")\n",
    "#             pass\n",
    "\n",
    "# # 计算相关系数\n",
    "# correlations = {}\n",
    "\n",
    "# for column in df1.columns:\n",
    "#     if column in df2.columns:\n",
    "#         #pandas的corr自动忽略Nan值\n",
    "#         #correlations数据结构为{‘站名’:[相关系数, 样本数]}\n",
    "#         correlations[column] = [ df1[column].corr(df2[column]), df1[column].notnull().sum()]\n",
    "\n",
    "# cc_csv = pd.DataFrame(columns=['站名','相关系数','样本数'])\n",
    "\n",
    "# # 打印相关系数\n",
    "# for site, corr in correlations.items():\n",
    "#     #相关系数＞0.6可以接受，样本数较少是因为smap基本只有2015年以后的数据，这样situ的样本数就会少很多,说明2015以前的situ数据可以用\n",
    "#     #if corr[0] > 0.6:\n",
    "#     print(f\"站点{site}的相关系数为: {corr[0]}, 样本数为: {corr[1]}\")\n",
    "#     cc_csv = cc_csv._append({'站名':site,'相关系数':corr[0],'样本数':corr[1]},ignore_index=True)\n",
    "\n",
    "# #cc_csv.to_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\ismn_smap_yellow_cc_all.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #计算  smap_newsites vs. era5/situ_newsites/situ_ismn cc相关系数\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # 读取csv文件\n",
    "# # df1 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\newsites_yellow_result.csv\", index_col=0)\n",
    "# # df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\smap_newsites.csv\", index_col=0)\n",
    "# # df1 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\ismn_yellow_result.csv\", index_col=0)\n",
    "# # df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\smap_ismnsites.csv\", index_col=0)\n",
    "# # df1 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\ismn_yellow_result.csv\", index_col=0)\n",
    "# # df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\smap_ismnsites.csv\", index_col=0)\n",
    "# df1 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\newsites_yellow_result.csv\", index_col=0)\n",
    "# df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\era5_newsites.csv\", index_col=0)\n",
    "\n",
    "# # 将索引转换为datetime类型\n",
    "# df1.index = pd.to_datetime(df1.index, format='mixed')\n",
    "# df2.index = pd.to_datetime(df2.index, format='mixed')\n",
    "\n",
    "# # 舍去时间戳的时刻部分，只保留日期\n",
    "# df1.index = df1.index.date\n",
    "# df2.index = df2.index.date\n",
    "\n",
    "# # 你也可以将索引再转换为datetime类型\n",
    "# df1.index = pd.to_datetime(df1.index)\n",
    "# df2.index = pd.to_datetime(df2.index)\n",
    "# #print(df1.index)\n",
    "\n",
    "# # df1.index = pd.to_datetime(df1.index, format='%Y/%m/%d')\n",
    "# df1.index = pd.to_datetime(df1.index, format='mixed')\n",
    "# df2.index = pd.to_datetime(df2.index, format='mixed')\n",
    "\n",
    "# # 保证取值在相同时间索引下\n",
    "# df1 = df1[df1.index.isin(df2.index)]\n",
    "# df2 = df2[df2.index.isin(df1.index)]\n",
    "\n",
    "# # 打印每个时间索引下的每个站点的数值\n",
    "# for time_index in df1.index:\n",
    "#     #print(f\"时间索引：{time_index}\")\n",
    "#     for column in df1.columns:\n",
    "#         if column in df2.columns:\n",
    "#             #print(f\"站点{column}在文件1中的数值为: {df1.at[time_index, column]}，在文件2中的数值为: {df2.at[time_index, column]}\")\n",
    "#             pass\n",
    "\n",
    "# # 计算相关系数\n",
    "# correlations = {}\n",
    "\n",
    "# for column in df1.columns:\n",
    "#     if column in df2.columns:\n",
    "#         #pandas的corr自动忽略Nan值\n",
    "#         #correlations数据结构为{‘站名’:[相关系数, 样本数]}\n",
    "#         correlations[column] = [ df1[column].corr(df2[column]), df1[column].notnull().sum()]\n",
    "\n",
    "# cc_csv = pd.DataFrame(columns=['站名','经度','纬度','相关系数','样本数'])\n",
    "# #具体的站名、经纬度文件\n",
    "# df_loc = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\newsites_yellow_points.csv\", index_col=0)\n",
    "\n",
    "\n",
    "# # 打印相关系数\n",
    "# for site, corr in correlations.items():\n",
    "#     #相关系数＞0.6可以接受，样本数较少是因为smap基本只有2015年以后的数据，这样situ的样本数就会少很多,说明2015以前的situ数据可以用\n",
    "#     if corr[0] > 0.6:\n",
    "#         print(f\"站点{site}的相关系数为: {corr[0]}, 样本数为: {corr[1]}\")\n",
    "#         #取出指定站名的经纬度\n",
    "#         location = df_loc.loc[df_loc['站名'] == site, ['经度','纬度']]\n",
    "#         #selected_row_data = df[df['Column1'] == selected_value][['Column2', 'Column3']]\n",
    "#         #loction = df[df['Column1'] == selected_value][['经度', '纬度']]\n",
    "#         cc_csv = cc_csv._append({'站名':site,'经度':location.iloc[0].at['经度'],'纬度':location.iloc[0].at['纬度'],'相关系数':corr[0],'样本数':corr[1]},ignore_index=True)\n",
    "\n",
    "# cc_csv.to_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\newsites_era5_newsites_ccover0_6.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # old version cal cc\n",
    "# #ERA5-SM vs. new sites\n",
    "# #对于每个站点\n",
    "# #提取时间点对应的分别到insite_list   era5_list\n",
    "# #perseasion\n",
    "\n",
    "# #yellow_sm_merged.csv[站号、站名、经纬度、时间、10厘米深度重量含水率]\n",
    "# import pandas as pd\n",
    "# from scipy.stats import pearsonr\n",
    "# data = pd.read_csv(\"yellow_sm_merged.csv\")\n",
    "\n",
    "# site_kv_record = {}\n",
    "# era5_kv_record = {}\n",
    "# #对于每一年的nc文件\n",
    "# for year in range(2000,2022):\n",
    "#     nc=Dataset(r'D:\\dataset\\ERA5_yellow_800\\\\%s_yellow.nc' % (year) )\n",
    "#     #nc.variables['longitude'][:].data == long_new and nc.variables['longitude'][:].data == lati_new and time_new==1 : \n",
    "\n",
    "#     #对于合并后的pd文件的每一行\n",
    "#     for index, row in data.iterrows():\n",
    "#         if row['站名'] not in site_kv_record:\n",
    "#             insite_list = []\n",
    "#             site_kv_record.update({row['站名'] : insite_list})\n",
    "#             print(\"site添加新站点：\",row['站名'])\n",
    "#         if row['站名'] not in era5_kv_record:\n",
    "#             era5_list = []\n",
    "#             era5_kv_record.update({row['站名'] : era5_list})\n",
    "#             print(\"era5添加新站点：\",row['站名'])\n",
    "\n",
    "#         sm_site = row['10厘米深度重量含水率'] / 100\n",
    "#         #site_kv_record[row['站名']].append(sm_site)  #append insite_list\n",
    "\n",
    "#         long = row['经度']\n",
    "#         #long_list = long.split('.')\n",
    "#         #long_new = long_list[0] + '.' + long_list[1][:1]\n",
    "#         #long_corr = int((float(long_new) - 95.0) / 0.1)  #对应下标\n",
    "#         long_corr = np.argmin(abs(long - nc.variables['longitude'][:] ))\n",
    "\n",
    "#         lati = row['纬度']\n",
    "#         #lati_list = lati.split('.')\n",
    "#         #lati_new = lati_list[0] + '.' + lati_list[1][:1]\n",
    "#         #lati_corr = int((42.0 - float(lati_new)) / 0.1)\n",
    "#         lati_corr = np.argmin(abs(lati - nc.variables['latitude'][:] ))\n",
    "\n",
    "#         time = row['时间']\n",
    "#         time_new = time.split('/')\n",
    "#         targetDay = datetime.date(int(time_new[0]), int(time_new[1]), int(time_new[2].split( )[0]))  #将输入的日期格式化成标准的日期\n",
    "#         mid_targetDay = (targetDay - datetime.date(targetDay.year, 1, 1)).days  #减去上一年最后一天，可得解\n",
    "#         day_index = int(mid_targetDay)  #提取整数天数\n",
    "#         if year == int(time_new[0]):\n",
    "#             site_kv_record[row['站名']].append(sm_site)  #append insite_list\n",
    "#             sm_era5=nc.variables['swvl1'][day_index][lati_corr][long_corr]\n",
    "#             era5_kv_record[row['站名']].append(sm_era5)  #append era5_list\n",
    "\n",
    "\n",
    "# cc_list = []\n",
    "# for key_site,value_site in site_kv_record.items():\n",
    "#     for key_era5,value_era5 in era5_kv_record.items():\n",
    "#         if key_era5 == key_site:\n",
    "#             print(len(era5_kv_record[key_era5]))\n",
    "#             print(len(site_kv_record[key_site]))\n",
    "#             cc, p = pearsonr(era5_kv_record[key_era5], site_kv_record[key_site])\n",
    "\n",
    "#             cc_list.append(cc)\n",
    "#             print(key_era5,\"-相关系数：\",cc)\n",
    "\n",
    "# print(\"平均cc:\",np.mean(cc_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # 从全部站点中，筛选出在黄河流域内的站点\n",
    "# # 这里好多bug，直接用arcgis掩膜裁剪了\n",
    "# import pandas as pd\n",
    "# import geopandas as gpd\n",
    "# from shapely.geometry import Point\n",
    "\n",
    "# # 读取黄河流域的 Shapefile 文件\n",
    "# shapefile_path = r'D:/dataset/流域gis/huang_river.shp'\n",
    "# huanghe_shape = gpd.read_file(shapefile_path)\n",
    "\n",
    "# # 读取站点位置的 CSV 文件\n",
    "# csv_file_path =  r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\new_new_sites_points.csv\"\n",
    "# station_data = pd.read_csv(csv_file_path)\n",
    "\n",
    "\n",
    "# # 将站点数据转换为 GeoDataFrame，创建 Point 对象\n",
    "# geometry = [Point(lon, lat) for lon, lat in zip(station_data['lon'], station_data['lat'])]\n",
    "# stations_gdf = gpd.GeoDataFrame(station_data, geometry=geometry, crs='EPSG:4326')\n",
    "\n",
    "# # 使用 GeoPandas 的 spatial join 进行空间连接，提取流域内的点\n",
    "# points_within_huanghe = gpd.overlay(stations_gdf, huanghe_shape, how='intersection')\n",
    "\n",
    "# # 提取结果中的点数据\n",
    "# output_csv_file = 'path_to_output_csv_file/points_within_huanghe.csv'\n",
    "# points_within_huanghe.to_csv(output_csv_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算  era5 vs. situ_new_newsites cc相关系数\n",
    "# 使用后删除\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 读取csv文件\n",
    "# df1 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\newsites_yellow_result.csv\", index_col=0)\n",
    "# df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\smap_newsites.csv\", index_col=0)\n",
    "# df1 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\ismn_yellow_result.csv\", index_col=0)\n",
    "# df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\smap_ismnsites.csv\", index_col=0)\n",
    "# df1 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\ismn_yellow_result.csv\", index_col=0)\n",
    "# df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\smap_ismnsites.csv\", index_col=0)\n",
    "df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\new_new_sites_all_result.csv\", index_col=0)\n",
    "df1 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\era5_yellow_new_newsites.csv\", index_col=0)\n",
    "\n",
    "# 将索引转换为datetime类型\n",
    "df1.index = pd.to_datetime(df1.index, format='mixed')\n",
    "\n",
    "# 舍去时间戳的时刻部分，只保留日期\n",
    "df1.index = df1.index.date\n",
    "\n",
    "# 你也可以将索引再转换为datetime类型\n",
    "df1.index = pd.to_datetime(df1.index)\n",
    "#print(df1.index)\n",
    "\n",
    "# df1.index = pd.to_datetime(df1.index, format='%Y/%m/%d')\n",
    "df1.index = pd.to_datetime(df1.index, format='mixed')\n",
    "df2.index = pd.to_datetime(df2.index, format='mixed')\n",
    "\n",
    "# 保证取值在相同时间索引下\n",
    "df1 = df1[df1.index.isin(df2.index)]\n",
    "df2 = df2[df2.index.isin(df1.index)]\n",
    "\n",
    "# 打印每个时间索引下的每个站点的数值\n",
    "for time_index in df1.index:\n",
    "    #print(f\"时间索引：{time_index}\")\n",
    "    for column in df1.columns:\n",
    "        if column in df2.columns:\n",
    "            #print(f\"站点{column}在文件1中的数值为: {df1.at[time_index, column]}，在文件2中的数值为: {df2.at[time_index, column]}\")\n",
    "            pass\n",
    "\n",
    "# 计算相关系数\n",
    "correlations = {}\n",
    "\n",
    "for column in df1.columns:\n",
    "    if column in df2.columns:\n",
    "        #pandas的corr自动忽略Nan值\n",
    "        #correlations数据结构为{‘站名’:[相关系数, 样本数]}\n",
    "        correlations[column] = [ df1[column].corr(df2[column]), df2[column].notnull().sum()]\n",
    "\n",
    "cc_csv = pd.DataFrame(columns=['站名','相关系数','样本数'])\n",
    "\n",
    "# 打印相关系数\n",
    "for site, corr in correlations.items():\n",
    "    #相关系数＞0.6可以接受，样本数较少是因为smap基本只有2015年以后的数据，这样situ的样本数就会少很多,说明2015以前的situ数据可以用\n",
    "    #if corr[0] > 0.6:\n",
    "    print(f\"站点{site}的相关系数为: {corr[0]}, 样本数为: {corr[1]}\")\n",
    "    cc_csv = cc_csv._append({'站名':site,'相关系数':corr[0],'样本数':corr[1]},ignore_index=True)\n",
    "\n",
    "#cc_csv.to_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\ismn_smap_yellow_cc_all.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #从new_new_sites全部站点【数据】中，筛选出在黄河流域内的站点\n",
    "# #注意不是从站点--->站点，是从全站点数据--->流域内站点数据\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # 读取new_new_sites_all_result.csv文件\n",
    "# all_result_df = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\new_new_sites_all_result.csv\")\n",
    "\n",
    "# # 读取new_new_sites_yellow_points.csv文件\n",
    "# yellow_points_df = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\new_new_sites_yellow_points.csv\")\n",
    "\n",
    "# # 获取需要的站号列表\n",
    "# needed_station_ids = yellow_points_df['Station_ID'].tolist()\n",
    "\n",
    "# # 使用列表推导式将所有元素转换为字符串类型\n",
    "# str_list = [str(item) for item in needed_station_ids]\n",
    "\n",
    "# #保留时间列（这里好像不是行索引，是一个单独的列，原索引为0，1，2....）\n",
    "# str_list.insert(0, 'Time')\n",
    "\n",
    "# # 将所选列添加到新的DataFrame中\n",
    "# selected_df = all_result_df.loc[:, str_list]\n",
    "\n",
    "# # 将结果保存为新的CSV文件\n",
    "# selected_df.to_csv('new_new_sites_yellow_result.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 下载ESACCI nc数据\n",
    "# # some bugs\n",
    "# import os\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# # 指定要下载的基础URL\n",
    "# base_url = \"https://data.ceda.ac.uk/neodc/esacci/soil_moisture/data/daily_files/COMBINED/v07.1/\"\n",
    "# # base_url = \"https://www.baidu.com/\"\n",
    "\n",
    "# # 创建一个目录来存储下载的数据\n",
    "# data_directory = \"ESACCI\"\n",
    "# os.makedirs(data_directory, exist_ok=True)\n",
    "\n",
    "# # # 禁用代理\n",
    "# # os.environ['NO_PROXY'] = 'data.ceda.ac.uk'\n",
    "\n",
    "# # 循环遍历每个年份\n",
    "# for year in range(1978, 2022):  # 更改范围以适应您的需求\n",
    "#     year_url = f\"{base_url}{year}\"\n",
    "\n",
    "#     # 创建一个目录来存储当前年份的数据\n",
    "#     year_directory = os.path.join(data_directory, str(year))\n",
    "#     os.makedirs(year_directory, exist_ok=True)\n",
    "\n",
    "#     # 获取年份页面的内容\n",
    "#     response = requests.get(year_url)\n",
    "#     print('get success!')\n",
    "#     if response.status_code == 200:\n",
    "#         print('response success!')\n",
    "#         soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "#         # 查找所有链接以.nc结尾的文件链接\n",
    "#         links = soup.find_all(\"a\", href=lambda href: href and href.endswith(\".nc\"))\n",
    "\n",
    "#         # 下载每个链接对应的文件\n",
    "#         for link in links:\n",
    "#             file_url = link[\"href\"]\n",
    "#             file_name = os.path.basename(file_url)\n",
    "\n",
    "#             # 构建完整的文件路径\n",
    "#             file_path = os.path.join(year_directory, file_name)\n",
    "\n",
    "#             # 下载文件\n",
    "#             file_response = requests.get(file_url)\n",
    "#             if file_response.status_code == 200:\n",
    "#                 with open(file_path, \"wb\") as file:\n",
    "#                     file.write(file_response.content)\n",
    "#                 print(f\"Downloaded: {file_name}\")\n",
    "#             else:\n",
    "#                 print(f\"Failed to download: {file_name}\")\n",
    "#     else:\n",
    "#         print(f\"Failed to access year {year} data\")\n",
    "\n",
    "# print(\"Download completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#正确的\n",
    "#根据站点经纬度提取ESACCI数据\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# 定义站点数据文件路径\n",
    "station_csv_file = 'C:/Users/pc/Desktop/嗑盐/Y_data/new_new_sites_yellow_points.csv'\n",
    "\n",
    "# 定义nc文件夹路径\n",
    "nc_folder = r'D:\\dataset\\ESACCI_test'\n",
    "\n",
    "# 从CSV文件读取站点数据\n",
    "station_data = pd.read_csv(station_csv_file)\n",
    "\n",
    "# 创建一个空的DataFrame来存储提取后的数据，行索引为时间，列名为站点名\n",
    "extracted_data = pd.DataFrame()\n",
    "\n",
    "# 遍历每个站点\n",
    "for station_row in tqdm(station_data.itertuples(), total=len(station_data), desc='Processing Stations'):\n",
    "    station_lon = station_row.lon\n",
    "    station_lat = station_row.lat\n",
    "    station_name = station_row.Station_ID\n",
    "    mid_data = pd.DataFrame()\n",
    "    # 遍历每个nc文件 即每个年份\n",
    "    for year_file in os.listdir(nc_folder):\n",
    "        for nc_file in os.listdir(os.path.join(nc_folder,year_file)):\n",
    "            if nc_file.endswith('.nc'):\n",
    "                # 从文件名中提取年份\n",
    "                year = int(nc_file.split('-')[-2][:4])\n",
    "\n",
    "                # 使用xarray库读取nc文件\n",
    "                ds = xr.open_dataset(os.path.join(nc_folder, year_file, nc_file))\n",
    "\n",
    "                # 提取longitude、latitude、time和swvl1变量\n",
    "                longitude = ds['lon'].values\n",
    "                latitude = ds['lat'].values\n",
    "                time = ds['time'].values\n",
    "\n",
    "                \n",
    "                swvl1 = ds['sm'].values\n",
    "\n",
    "                # 找到最近的nc坐标索引\n",
    "                lon_ind = (abs(longitude - station_lon)).argmin()\n",
    "                lat_ind = (abs(latitude - station_lat)).argmin()\n",
    "\n",
    "                # 获取对应的swvl1值\n",
    "                swvl1_values = swvl1[:, lat_ind, lon_ind]\n",
    "\n",
    "                # 创建一个以时间为索引的DataFrame\n",
    "                # 一次取一个站点的一天数据【即一小列数据】\n",
    "                df = pd.DataFrame(swvl1_values, index=pd.to_datetime(time, format='%Y-%m-%dT%H:%M:%S'), columns=[station_name])\n",
    "\n",
    "                # 将当前站点的数据合并到提取后的数据中\n",
    "                # for nc_file循环后（1年内的全部nc），mid_data是一个站点的一年数据\n",
    "                if mid_data.empty:\n",
    "                    mid_data = df\n",
    "                else:\n",
    "                    mid_data = pd.concat([mid_data, df], axis=0, ignore_index=False)\n",
    "\n",
    "                # 关闭xarray数据集\n",
    "                ds.close()\n",
    "    # extracted_data是\n",
    "    if extracted_data.empty:\n",
    "        extracted_data = mid_data\n",
    "    else:\n",
    "        extracted_data = pd.concat([extracted_data, mid_data], axis=1)\n",
    "# 保存提取后的数据到CSV文件\n",
    "extracted_data.to_csv('ESACCI_yellow_new_newsites.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据站点经纬度提取ESACCI数据\n",
    "# 重构一下逻辑，哪些是高io操作，放在外面\n",
    "# 使用后删除\n",
    "\n",
    "#正确的\n",
    "#根据站点经纬度提取ESACCI数据\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# 定义站点数据文件路径\n",
    "station_csv_file = 'C:/Users/pc/Desktop/嗑盐/Y_data/newsites_yellow_points.csv'\n",
    "\n",
    "# 定义nc文件夹路径\n",
    "nc_folder = r'D:\\dataset\\ESACCI'\n",
    "\n",
    "\n",
    "\n",
    "# 从CSV文件读取站点数据\n",
    "station_data = pd.read_csv(station_csv_file)\n",
    "\n",
    "\n",
    "# 遍历每个nc文件 即每个年份\n",
    "# final_data 存储每一年全部站点的数据\n",
    "# 最终想要的当然要放在循环外面\n",
    "#final_data = pd.DataFrame()\n",
    "# for year_file in tqdm(os.listdir(nc_folder), total=len(os.listdir(nc_folder)), desc='Processing each year'):\n",
    "final_data = pd.DataFrame()\n",
    "for year_file in tqdm(os.listdir(nc_folder), position=0, desc=\"each year\", leave=False, colour='green', ncols=80):\n",
    "    for nc_file in tqdm(os.listdir(os.path.join(nc_folder,year_file)), position=0, desc=\"each items\", leave=False, colour='green', ncols=80):\n",
    "        # 从文件名中提取年份\n",
    "        year = int(nc_file.split('-')[-2][:4])\n",
    "\n",
    "        # 使用xarray库读取nc文件\n",
    "        ds = xr.open_dataset(os.path.join(nc_folder, year_file, nc_file))\n",
    "\n",
    "        # 提取longitude、latitude、time和swvl1变量\n",
    "        longitude = ds['lon'].values\n",
    "        latitude = ds['lat'].values\n",
    "        time = ds['time'].values\n",
    "\n",
    "        swvl1 = ds['sm'].values\n",
    "        \n",
    "        # 关闭xarray数据集\n",
    "        ds.close()\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        # oneday_allsites_data 存储每一天全部站点的数据【即每一小行数据】\n",
    "        oneday_allsites_data = pd.DataFrame()\n",
    "        # for station_row in tqdm(station_data.itertuples(), total=len(station_data), desc='Processing Stations'):\n",
    "        for station_row in station_data.itertuples():\n",
    "            station_lon = station_row.经度\n",
    "            station_lat = station_row.纬度\n",
    "            station_name = station_row.站名\n",
    "            # 找到最近的nc坐标索引\n",
    "            lon_ind = (abs(longitude - station_lon)).argmin()\n",
    "            lat_ind = (abs(latitude - station_lat)).argmin()\n",
    "\n",
    "            # 获取对应的swvl1值\n",
    "            swvl1_values = swvl1[:, lat_ind, lon_ind]\n",
    "\n",
    "            # 创建一个以时间为索引的DataFrame\n",
    "            # 一次取一个站点的一天数据【即一小列数据】\n",
    "            df = pd.DataFrame(swvl1_values, index=pd.to_datetime(time, format='%Y-%m-%dT%H:%M:%S'), columns=[station_name])\n",
    "\n",
    "            # 将当前站点的数据合并到提取后的数据中\n",
    "            # for nc_file循环后（1年内的全部nc），mid_data是一个站点的一年数据\n",
    "            if oneday_allsites_data.empty:\n",
    "                oneday_allsites_data = df\n",
    "            else:\n",
    "                # 左右拼接[就是一行(一天)]\n",
    "                oneday_allsites_data = pd.concat([oneday_allsites_data, df], axis=1)\n",
    "            \n",
    "            #print(oneday_allsites_data)\n",
    "        if final_data.empty:\n",
    "            final_data = oneday_allsites_data\n",
    "        else:\n",
    "            # 上下拼接\n",
    "            final_data = pd.concat([final_data, oneday_allsites_data], axis=0, ignore_index=False)\n",
    "        # # 关闭xarray数据集\n",
    "        # ds.close()\n",
    "# 保存提取后的数据到CSV文件\n",
    "final_data.to_csv('ESACCI_ismnsites.csv')\n",
    "# final_data.to_csv('ESACCI_yellow_new_newsites.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "站点52797的相关系数为: 0.674784647539042, 样本数为: 6143\n",
      "站点52895的相关系数为: 0.6653797107740647, 样本数为: 6549\n",
      "站点52896的相关系数为: 0.6423463886406474, 样本数为: 6351\n",
      "站点52980的相关系数为: 0.6017065180068442, 样本数为: 6376\n",
      "站点52988的相关系数为: 0.603052431937029, 样本数为: 6417\n",
      "站点52993的相关系数为: 0.6093925874322013, 样本数为: 6991\n",
      "站点52995的相关系数为: 0.6061307989387243, 样本数为: 6640\n",
      "站点53357的相关系数为: 0.6382997569513126, 样本数为: 5604\n",
      "站点53368的相关系数为: 0.6082069399249309, 样本数为: 5227\n",
      "站点53455的相关系数为: 0.6289515998260424, 样本数为: 6717\n",
      "站点53478的相关系数为: 0.6004818402279386, 样本数为: 6495\n",
      "站点53519的相关系数为: 0.6104584246056384, 样本数为: 6733\n",
      "站点53547的相关系数为: 0.6132507232887296, 样本数为: 7318\n",
      "站点53646的相关系数为: 0.6270871551886481, 样本数为: 7349\n",
      "站点53704的相关系数为: 0.6200972565236778, 样本数为: 7168\n",
      "站点53705的相关系数为: 0.6652450102952091, 样本数为: 6822\n",
      "站点53707的相关系数为: 0.6447382817010755, 样本数为: 6545\n",
      "站点53723的相关系数为: 0.6777010793283681, 样本数为: 7034\n",
      "站点53725的相关系数为: 0.7113863331486165, 样本数为: 7258\n",
      "站点53735的相关系数为: 0.6658226540669806, 样本数为: 6423\n",
      "站点53740的相关系数为: 0.6852247705905125, 样本数为: 7184\n",
      "站点53750的相关系数为: 0.6283559386766578, 样本数为: 7501\n",
      "站点53751的相关系数为: 0.620704110500181, 样本数为: 7916\n",
      "站点53754的相关系数为: 0.6222756019860886, 样本数为: 7916\n",
      "站点53806的相关系数为: 0.6438323799193338, 样本数为: 7246\n",
      "站点53810的相关系数为: 0.6820960180113667, 样本数为: 7233\n",
      "站点53821的相关系数为: 0.6099071919742693, 样本数为: 7545\n",
      "站点53829的相关系数为: 0.6033219785378546, 样本数为: 9202\n",
      "站点53881的相关系数为: 0.6616365667690386, 样本数为: 6653\n",
      "站点53917的相关系数为: 0.6358312314113878, 样本数为: 7224\n",
      "站点53923的相关系数为: 0.6552376043674651, 样本数为: 9208\n",
      "站点53924的相关系数为: 0.6111786903716404, 样本数为: 8563\n",
      "站点53925的相关系数为: 0.64777134290609, 样本数为: 8900\n",
      "站点53926的相关系数为: 0.6486919653941795, 样本数为: 9724\n",
      "站点53928的相关系数为: 0.6050334730572703, 样本数为: 9662\n",
      "站点53929的相关系数为: 0.6704716498828986, 样本数为: 9690\n",
      "站点53937的相关系数为: 0.6509616248284386, 样本数为: 9759\n",
      "站点53938的相关系数为: 0.6191381859011618, 样本数为: 8715\n",
      "站点53948的相关系数为: 0.6448165482037194, 样本数为: 9407\n",
      "站点53949的相关系数为: 0.6439227468372848, 样本数为: 9983\n",
      "站点53955的相关系数为: 0.6117066207041779, 样本数为: 8159\n",
      "站点53956的相关系数为: 0.6452119070166413, 样本数为: 8267\n",
      "站点53959的相关系数为: 0.6230311467009667, 样本数为: 9784\n",
      "站点57001的相关系数为: 0.613550958046334, 样本数为: 7956\n",
      "站点57002的相关系数为: 0.6014694178354394, 样本数为: 8137\n",
      "站点57023的相关系数为: 0.6385806299862907, 样本数为: 9799\n",
      "站点57026的相关系数为: 0.643362238210387, 样本数为: 8250\n",
      "站点57029的相关系数为: 0.6629691855603588, 样本数为: 9278\n",
      "站点57030的相关系数为: 0.6887268388724991, 样本数为: 7004\n",
      "站点57033的相关系数为: 0.6004422822783314, 样本数为: 9195\n",
      "站点57034的相关系数为: 0.6289370235824937, 样本数为: 8569\n",
      "站点57035的相关系数为: 0.7027026466829157, 样本数为: 7004\n",
      "站点57041的相关系数为: 0.6071366017333184, 样本数为: 9195\n",
      "站点57053的相关系数为: 0.6255983075869869, 样本数为: 8635\n",
      "站点57074的相关系数为: 0.6333263777568592, 样本数为: 7269\n",
      "站点57079的相关系数为: 0.618220236121994, 样本数为: 7355\n",
      "站点57123的相关系数为: 0.6278207105222403, 样本数为: 8569\n"
     ]
    }
   ],
   "source": [
    "# #计算  ESACCI vs. situ_new_newsites cc相关系数\n",
    "# # 使用后删除\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # 读取csv文件\n",
    "# # df1 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\newsites_yellow_result.csv\", index_col=0)\n",
    "# # df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\smap_newsites.csv\", index_col=0)\n",
    "# # df1 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\ismn_yellow_result.csv\", index_col=0)\n",
    "# # df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\smap_ismnsites.csv\", index_col=0)\n",
    "# # df1 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\ismn_yellow_result.csv\", index_col=0)\n",
    "# # df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\smap_ismnsites.csv\", index_col=0)\n",
    "# # df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\smap_yellow_new_newsites.csv\", index_col=0)\n",
    "# df1 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\newsites_all_result.csv\", index_col=0)\n",
    "# df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\ESACCI_newsites.csv\", index_col=0)\n",
    "\n",
    "# # 将索引转换为datetime类型\n",
    "# df1.index = pd.to_datetime(df1.index, format='mixed')\n",
    "\n",
    "# # 舍去时间戳的时刻部分，只保留日期\n",
    "# df1.index = df1.index.date\n",
    "\n",
    "# # 你也可以将索引再转换为datetime类型\n",
    "# df1.index = pd.to_datetime(df1.index)\n",
    "# #print(df1.index)\n",
    "\n",
    "# # df1.index = pd.to_datetime(df1.index, format='%Y/%m/%d')\n",
    "# df1.index = pd.to_datetime(df1.index, format='mixed')\n",
    "# df2.index = pd.to_datetime(df2.index, format='mixed')\n",
    "\n",
    "# # 保证取值在相同时间索引下\n",
    "# df1 = df1[df1.index.isin(df2.index)]\n",
    "# df2 = df2[df2.index.isin(df1.index)]\n",
    "\n",
    "# # # 打印每个时间索引下的每个站点的数值\n",
    "# # for time_index in df1.index:\n",
    "# #     #print(f\"时间索引：{time_index}\")\n",
    "# #     for column in df1.columns:\n",
    "# #         if column in df2.columns:\n",
    "# #             #print(f\"站点{column}在文件1中的数值为: {df1.at[time_index, column]}，在文件2中的数值为: {df2.at[time_index, column]}\")\n",
    "# #             pass\n",
    "\n",
    "# # 计算相关系数\n",
    "# correlations = {}\n",
    "\n",
    "# # # 针对一个特殊情况\n",
    "# # # '57777.0'\n",
    "# # for column in df1.columns:\n",
    "# #     if column[:-2] in df2.columns:\n",
    "# #         #pandas的corr自动忽略Nan值\n",
    "# #         #correlations数据结构为{‘站名’:[相关系数, 样本数]}\n",
    "# #         nums = min(df1[column].notnull().sum(),df2[column[:-2]].notnull().sum())\n",
    "# #         correlations[column] = [ df1[column].corr(df2[column[:-2]]), nums]\n",
    "\n",
    "# for column in df1.columns:\n",
    "#     if column in df2.columns:\n",
    "#         #pandas的corr自动忽略Nan值\n",
    "#         #correlations数据结构为{‘站名’:[相关系数, 样本数]}\n",
    "#         nums = min(df1[column].notnull().sum(),df2[column].notnull().sum())\n",
    "#         correlations[column] = [ df1[column].corr(df2[column]), nums]\n",
    "\n",
    "# cc_csv = pd.DataFrame(columns=['站名','相关系数','样本数'])\n",
    "\n",
    "# # 打印相关系数\n",
    "# for site, corr in correlations.items():\n",
    "#     #相关系数＞0.6可以接受，样本数较少是因为smap基本只有2015年以后的数据，这样situ的样本数就会少很多,说明2015以前的situ数据可以用\n",
    "#     if corr[0] > 0.6:\n",
    "#         print(f\"站点{site}的相关系数为: {corr[0]}, 样本数为: {corr[1]}\")\n",
    "#         cc_csv = cc_csv._append({'站名':site,'相关系数':corr[0],'样本数':corr[1]},ignore_index=True)\n",
    "\n",
    "# cc_csv.to_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\ESACCI_era5_new_newsites_ccover0_6.csv\",index=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "下载任务已提交！\n"
     ]
    }
   ],
   "source": [
    "# # SMOS 下载代码\n",
    "# # 已知下载连接，并行化下载\n",
    "# import os\n",
    "# import requests\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# # 定义下载链接的文本文件路径\n",
    "# txt_file_path = r\"D:\\dataset\\smos\\socat_search_result_url_list_20230925-032428.txt\"\n",
    "\n",
    "# # 指定保存下载文件的文件夹路径\n",
    "# download_folder = r'D:\\dataset\\smos'\n",
    "\n",
    "# # 创建保存下载文件的文件夹\n",
    "# if not os.path.exists(download_folder):\n",
    "#     os.makedirs(download_folder)\n",
    "\n",
    "# # 定义下载函数\n",
    "# def download_file(url):\n",
    "#     try:\n",
    "#         file_name = os.path.join(download_folder, os.path.basename(url))\n",
    "        \n",
    "#         # 检查文件是否已经存在，如果存在则跳过下载\n",
    "#         if os.path.exists(file_name):\n",
    "#             print(f\"文件已存在，跳过下载: {file_name}\")\n",
    "#             return\n",
    "        \n",
    "#         response = requests.get(url)\n",
    "#         if response.status_code == 200:\n",
    "#             with open(file_name, 'wb') as output_file:\n",
    "#                 output_file.write(response.content)\n",
    "#             # print(f\"下载成功: {file_name}\")\n",
    "#         else:\n",
    "#             print(f\"下载失败: {url}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"下载出错: {url}, 错误信息: {str(e)}\")\n",
    "\n",
    "# # 打开包含下载链接的文本文件\n",
    "# with open(txt_file_path, 'r') as file:\n",
    "#     # 创建线程池，最大同时下载数为5（可以根据需要调整）\n",
    "#     with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "#         # 提交下载任务给线程池\n",
    "#         for line in file:\n",
    "#             url = line.strip()\n",
    "#             executor.submit(download_file, url)\n",
    "\n",
    "# print(\"下载任务已提交！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "站点范县的相关系数为: 0.4714977221823278, 样本数为: 515\n",
      "站点窄口的相关系数为: 0.5003227419041137, 样本数为: 504\n",
      "站点梨林的相关系数为: 0.39925287512971724, 样本数为: 258\n",
      "站点青沟的相关系数为: 0.5274538241989122, 样本数为: 472\n",
      "站点大沟口的相关系数为: 0.572486849463875, 样本数为: 505\n",
      "站点原阳的相关系数为: 0.38933820102213534, 样本数为: 507\n",
      "站点寺河的相关系数为: 0.5515920424239313, 样本数为: 510\n",
      "站点大车集的相关系数为: 0.178551626257874, 样本数为: 503\n",
      "站点站街的相关系数为: 0.005782417935550611, 样本数为: 497\n",
      "站点柳庄的相关系数为: 0.6763154396861113, 样本数为: 66\n",
      "站点朱付村的相关系数为: 0.4339838594058723, 样本数为: 508\n",
      "站点陶花店的相关系数为: 0.4699916234921609, 样本数为: 506\n",
      "站点濮阳的相关系数为: 0.3089336795344481, 样本数为: 512\n",
      "站点济源的相关系数为: 0.4948646614327105, 样本数为: 499\n",
      "站点长清的相关系数为: 0.2897058054271135, 样本数为: 214\n",
      "站点大汶口的相关系数为: 0.46172428757145273, 样本数为: 290\n",
      "站点黄前的相关系数为: 0.27026155232093163, 样本数为: 275\n",
      "站点楼德的相关系数为: 0.464638135985038, 样本数为: 281\n",
      "站点白楼的相关系数为: 0.39507228304121145, 样本数为: 154\n",
      "站点肥城的相关系数为: 0.44781215520363593, 样本数为: 157\n",
      "站点安塞的相关系数为: 0.3698343701065013, 样本数为: 117\n",
      "站点枣园的相关系数为: 0.4309316979258201, 样本数为: 124\n",
      "站点榆林的相关系数为: 0.27413865049817937, 样本数为: 142\n",
      "站点绥德的相关系数为: 0.32635990298432055, 样本数为: 146\n",
      "站点米脂的相关系数为: 0.3896472384334202, 样本数为: 145\n",
      "站点魏家堡的相关系数为: 0.49005246713910966, 样本数为: 161\n",
      "站点益门镇的相关系数为: 0.3492407276990483, 样本数为: 162\n",
      "站点黑峪口的相关系数为: 0.4865771741171514, 样本数为: 160\n",
      "站点涝峪口的相关系数为: 0.2987124362586511, 样本数为: 182\n",
      "站点罗李村的相关系数为: 0.3398898352495011, 样本数为: 235\n",
      "站点耀县的相关系数为: 0.5008078890781711, 样本数为: 163\n",
      "站点益店的相关系数为: 0.391482009905969, 样本数为: 161\n",
      "站点扶风的相关系数为: 0.49643690190135625, 样本数为: 161\n",
      "站点杨凌的相关系数为: 0.38444396611002873, 样本数为: 160\n",
      "站点王曲的相关系数为: 0.46950066912335325, 样本数为: 163\n",
      "站点富平的相关系数为: 0.4802239364104494, 样本数为: 163\n",
      "站点三原的相关系数为: 0.41675988702404604, 样本数为: 163\n",
      "站点固市的相关系数为: 0.4318175213159288, 样本数为: 356\n",
      "站点张家山的相关系数为: 0.4503071230762064, 样本数为: 163\n",
      "站点阡东的相关系数为: 0.44129796297880775, 样本数为: 163\n",
      "站点吴旗的相关系数为: 0.35234749460035775, 样本数为: 156\n",
      "站点交口河的相关系数为: 0.36132276715680406, 样本数为: 158\n",
      "站点状头的相关系数为: 0.3689848143674024, 样本数为: 163\n",
      "站点澄城的相关系数为: 0.4452526841486766, 样本数为: 163\n",
      "站点蒲城的相关系数为: 0.4151987871026129, 样本数为: 163\n",
      "站点大荔的相关系数为: 0.3689963868915846, 样本数为: 163\n",
      "站点灵口的相关系数为: 0.3102548690001872, 样本数为: 141\n",
      "站点洛南的相关系数为: 0.33382543200401155, 样本数为: 150\n",
      "站点涧里的相关系数为: 0.42013128901328745, 样本数为: 251\n",
      "站点卧虎山的相关系数为: 0.46642295879959533, 样本数为: 366\n",
      "站点平阴的相关系数为: 0.4476566297204678, 样本数为: 156\n",
      "站点北望的相关系数为: 0.42853320973742154, 样本数为: 301\n",
      "站点戴村坝的相关系数为: 0.46178002167182286, 样本数为: 303\n",
      "站点东周的相关系数为: 0.5543199351578311, 样本数为: 312\n",
      "站点红寺堡的相关系数为: 0.39421458728776604, 样本数为: 175\n",
      "站点盐池的相关系数为: 0.3970492425098101, 样本数为: 146\n",
      "站点王乐井的相关系数为: 0.4166578145634922, 样本数为: 177\n",
      "站点高沙窝的相关系数为: 0.4730125578229512, 样本数为: 183\n",
      "站点兴仁的相关系数为: 0.4339333448762988, 样本数为: 180\n",
      "站点彭阳的相关系数为: 0.4301511004765805, 样本数为: 873\n",
      "站点隆德的相关系数为: 0.29469174029026185, 样本数为: 1448\n",
      "站点三关口的相关系数为: 0.28039353685004875, 样本数为: 102\n",
      "站点黄家河的相关系数为: 0.27052219295953606, 样本数为: 950\n",
      "站点原州的相关系数为: 0.5516487795170723, 样本数为: 114\n",
      "站点韩府湾（三）的相关系数为: 0.1822456589439043, 样本数为: 1288\n",
      "站点贺堡的相关系数为: 0.44067841992746837, 样本数为: 154\n",
      "站点刘瑶的相关系数为: 0.6502773549007355, 样本数为: 246\n",
      "站点孟县的相关系数为: 0.25818433516466344, 样本数为: 248\n",
      "站点温县的相关系数为: 0.40593059057265896, 样本数为: 243\n",
      "站点何营的相关系数为: 0.29862045556702016, 样本数为: 242\n",
      "站点封丘的相关系数为: 0.27305738757744297, 样本数为: 243\n",
      "站点马楼的相关系数为: 0.5147476349129212, 样本数为: 240\n",
      "站点卢氏的相关系数为: 0.6520855912858274, 样本数为: 200\n",
      "站点渭源的相关系数为: 0.3573935281996316, 样本数为: 210\n",
      "站点定西西河的相关系数为: 0.3994086535593641, 样本数为: 154\n",
      "站点何家坡的相关系数为: 0.5745237913895583, 样本数为: 162\n",
      "站点郭城驿的相关系数为: 0.23405375166191358, 样本数为: 133\n",
      "站点李家村的相关系数为: 0.4157889169296359, 样本数为: 187\n",
      "站点红旗的相关系数为: 0.38421756834755516, 样本数为: 187\n",
      "站点折桥河道的相关系数为: 0.25388230482824653, 样本数为: 192\n",
      "站点碌曲的相关系数为: 0.39457793409038705, 样本数为: 188\n",
      "站点平凉的相关系数为: 0.18877728754044998, 样本数为: 192\n",
      "站点蔡家庙的相关系数为: 0.3942308376807123, 样本数为: 144\n",
      "站点三门峡的相关系数为: 0.5124595685791725, 样本数为: 301\n",
      "站点孟津的相关系数为: 0.6259375029710572, 样本数为: 162\n",
      "站点渑池的相关系数为: 0.6802647947060082, 样本数为: 301\n",
      "站点义马的相关系数为: 0.4743859808527241, 样本数为: 300\n",
      "站点栾川的相关系数为: 0.21692609023553247, 样本数为: 162\n",
      "站点段家沟的相关系数为: 0.6185182269027086, 样本数为: 503\n",
      "站点王团的相关系数为: 0.5974736315777852, 样本数为: 96\n",
      "站点西吉的相关系数为: 0.5737314764646618, 样本数为: 91\n",
      "站点蒿店的相关系数为: 0.44775805548084263, 样本数为: 89\n",
      "站点喊叫水的相关系数为: 0.4020545110905631, 样本数为: 90\n",
      "站点予旺的相关系数为: 0.5057567161772213, 样本数为: 93\n",
      "站点红羊的相关系数为: 0.3636840801004995, 样本数为: 92\n",
      "站点炭山的相关系数为: 0.026865881117556136, 样本数为: 261\n",
      "站点夏寨的相关系数为: 0.5565524075127406, 样本数为: 93\n",
      "站点偏关的相关系数为: 0.616997699375135, 样本数为: 302\n",
      "站点岢岚的相关系数为: 0.5298531939423292, 样本数为: 284\n",
      "站点圪洞的相关系数为: 0.5490326654197883, 样本数为: 302\n",
      "站点万年饱的相关系数为: 0.5396068046041098, 样本数为: 299\n",
      "站点裴沟的相关系数为: 0.5701981919178691, 样本数为: 87\n",
      "站点大宁的相关系数为: 0.7555194601227253, 样本数为: 94\n",
      "站点吉县的相关系数为: 0.7405424544461554, 样本数为: 97\n",
      "站点乡宁的相关系数为: 0.6281721208444384, 样本数为: 389\n",
      "站点曲峪的相关系数为: 0.34914008555258763, 样本数为: 259\n",
      "站点八角的相关系数为: 0.5056876433082537, 样本数为: 286\n",
      "站点吕庄的相关系数为: 0.6717237020438529, 样本数为: 380\n",
      "站点张留庄的相关系数为: 0.5036611786327874, 样本数为: 425\n",
      "站点大庙的相关系数为: 0.615812031855635, 样本数为: 387\n",
      "站点冯村的相关系数为: 0.5532495533228413, 样本数为: 408\n",
      "站点临晋的相关系数为: 0.4448011439905146, 样本数为: 413\n",
      "站点静乐的相关系数为: 0.5114220960145464, 样本数为: 296\n",
      "站点汾河二坝（二）的相关系数为: 0.46686018747118185, 样本数为: 399\n",
      "站点赵城的相关系数为: 0.6805652771846666, 样本数为: 386\n",
      "站点柴庄的相关系数为: 0.6347045081181766, 样本数为: 418\n",
      "站点董茹的相关系数为: 0.5263751639593044, 样本数为: 285\n",
      "站点芦家庄的相关系数为: 0.3934658601118739, 样本数为: 415\n",
      "站点松塔的相关系数为: 0.3232467456354781, 样本数为: 143\n",
      "站点盘陀的相关系数为: 0.6773233478585772, 样本数为: 310\n",
      "站点文峪河水库的相关系数为: 0.5281179998762462, 样本数为: 326\n",
      "站点双家寨的相关系数为: 0.5124832565345485, 样本数为: 155\n",
      "站点东庄的相关系数为: 0.5731222858274525, 样本数为: 340\n",
      "站点浍河水库的相关系数为: 0.5823498678118914, 样本数为: 477\n",
      "站点大交（续）的相关系数为: 0.5875336054710233, 样本数为: 372\n",
      "站点均衡场的相关系数为: 0.5557593728168889, 样本数为: 416\n",
      "站点翟店的相关系数为: 0.7763903628349861, 样本数为: 383\n",
      "站点皋落的相关系数为: 0.4514282352973838, 样本数为: 414\n",
      "站点孔家坡的相关系数为: 0.4792325444425222, 样本数为: 575\n",
      "站点泉眼山（二）的相关系数为: 0.12333420228394082, 样本数为: 1097\n",
      "站点鸣沙洲（四）的相关系数为: -0.3178190779628084, 样本数为: 122\n",
      "站点飞岭的相关系数为: 0.5402662810572397, 样本数为: 369\n",
      "站点油房的相关系数为: 0.5473962458246968, 样本数为: 425\n",
      "站点赤石桥的相关系数为: 0.6025578137619789, 样本数为: 411\n",
      "站点芹池的相关系数为: 0.2588554537991517, 样本数为: 408\n",
      "站点下冯庄的相关系数为: -0.18019837192763757, 样本数为: 352\n",
      "站点郭城驿(二)的相关系数为: 0.2825077787392446, 样本数为: 52\n",
      "站点义棠的相关系数为: 0.5639978256391459, 样本数为: 358\n"
     ]
    }
   ],
   "source": [
    "# 使用后删除\n",
    "# 计算  era5 vs. situ_ismn cc相关系数\n",
    "# 使用后删除\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 读取csv文件\n",
    "# df1 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\newsites_yellow_result.csv\", index_col=0)\n",
    "# df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\smap_newsites.csv\", index_col=0)\n",
    "# df1 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\ismn_yellow_result.csv\", index_col=0)\n",
    "# df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\smap_ismnsites.csv\", index_col=0)\n",
    "# df1 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\ismn_yellow_result.csv\", index_col=0)\n",
    "# df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\smap_ismnsites.csv\", index_col=0)\n",
    "# df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\new_new_sites_all_result.csv\", index_col=0)\n",
    "# df1 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\era5_yellow_new_newsites.csv\", index_col=0)\n",
    "df1 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\newsites_all_result.csv\", index_col=0)\n",
    "df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\ESACCI_newsites.csv\", index_col=0)\n",
    "\n",
    "# 将索引转换为datetime类型\n",
    "df1.index = pd.to_datetime(df1.index, format='mixed')\n",
    "\n",
    "# 舍去时间戳的时刻部分，只保留日期\n",
    "df1.index = df1.index.date\n",
    "\n",
    "# 你也可以将索引再转换为datetime类型\n",
    "df1.index = pd.to_datetime(df1.index)\n",
    "#print(df1.index)\n",
    "\n",
    "# df1.index = pd.to_datetime(df1.index, format='%Y/%m/%d')\n",
    "df1.index = pd.to_datetime(df1.index, format='mixed')\n",
    "df2.index = pd.to_datetime(df2.index, format='mixed')\n",
    "\n",
    "# 保证取值在相同时间索引下\n",
    "df1 = df1[df1.index.isin(df2.index)]\n",
    "df2 = df2[df2.index.isin(df1.index)]\n",
    "\n",
    "# 打印每个时间索引下的每个站点的数值\n",
    "for time_index in df1.index:\n",
    "    #print(f\"时间索引：{time_index}\")\n",
    "    for column in df1.columns:\n",
    "        if column in df2.columns:\n",
    "            #print(f\"站点{column}在文件1中的数值为: {df1.at[time_index, column]}，在文件2中的数值为: {df2.at[time_index, column]}\")\n",
    "            pass\n",
    "\n",
    "# 计算相关系数\n",
    "correlations = {}\n",
    "\n",
    "for column in df2.columns:\n",
    "    if column in df1.columns:\n",
    "        #pandas的corr自动忽略Nan值\n",
    "        #correlations数据结构为{‘站名’:[相关系数, 样本数]}\n",
    "        correlations[column] = [ df1[column].corr(df2[column]), df1[column].notnull().sum()]\n",
    "\n",
    "cc_csv = pd.DataFrame(columns=['站名','相关系数','样本数'])\n",
    "\n",
    "# 打印相关系数\n",
    "for site, corr in correlations.items():\n",
    "    #相关系数＞0.6可以接受，样本数较少是因为smap基本只有2015年以后的数据，这样situ的样本数就会少很多,说明2015以前的situ数据可以用\n",
    "    #if corr[0] > 0.6:\n",
    "    print(f\"站点{site}的相关系数为: {corr[0]}, 样本数为: {corr[1]}\")\n",
    "    cc_csv = cc_csv._append({'站名':site,'相关系数':corr[0],'样本数':corr[1]},ignore_index=True)\n",
    "\n",
    "#cc_csv.to_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\newsites_ESACCI_newsites_cc_all.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "变量名列表： dict_keys(['longitude', 'latitude', 'time', 'skt', 'tp', 'swvl1'])\n",
      "\n",
      "变量名: longitude\n",
      "units: degrees_east\n",
      "long_name: longitude\n",
      "\n",
      "变量名: latitude\n",
      "units: degrees_north\n",
      "long_name: latitude\n",
      "\n",
      "变量名: time\n",
      "units: hours since 1900-01-01 00:00:00.0\n",
      "long_name: time\n",
      "calendar: gregorian\n",
      "\n",
      "变量名: skt\n",
      "scale_factor: 0.0014110220172947217\n",
      "add_offset: 291.1932642155538\n",
      "_FillValue: -32767\n",
      "missing_value: -32767\n",
      "units: K\n",
      "long_name: Skin temperature\n",
      "\n",
      "变量名: tp\n",
      "scale_factor: 3.2154231462702705e-06\n",
      "add_offset: 0.10535655481069169\n",
      "_FillValue: -32767\n",
      "missing_value: -32767\n",
      "units: m\n",
      "long_name: Total precipitation\n",
      "\n",
      "变量名: swvl1\n",
      "scale_factor: 1.1688866215899814e-05\n",
      "add_offset: 0.3829973904301733\n",
      "_FillValue: -32767\n",
      "missing_value: -32767\n",
      "units: m**3 m**-3\n",
      "long_name: Volumetric soil water layer 1\n"
     ]
    }
   ],
   "source": [
    "# ERA5变量查看\n",
    "# \"D:\\dataset\\ERA5_yellow_800\\1950_yellow.nc\"\n",
    "\n",
    "import netCDF4 as nc\n",
    "\n",
    "# 打开NetCDF文件\n",
    "file_path = r\"D:\\dataset\\ERA5_yellow_800\\1950_yellow.nc\"  # 将'your_file.nc'替换为你的文件路径\n",
    "nc_file = nc.Dataset(file_path, 'r')\n",
    "\n",
    "# 获取文件中的变量名\n",
    "variable_names = nc_file.variables.keys()\n",
    "print(\"变量名列表：\", variable_names)\n",
    "\n",
    "# 遍历每个变量并查看属性\n",
    "for variable_name in variable_names:\n",
    "    variable = nc_file.variables[variable_name]\n",
    "    variable_attributes = variable.__dict__\n",
    "    print(f\"\\n变量名: {variable_name}\")\n",
    "    for key, value in variable_attributes.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# 关闭NetCDF文件\n",
    "nc_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算  smap_newsites vs. era5/situ_newsites/situ_ismn cc相关系数\n",
    "\n",
    "import pandas as pd\n",
    "# CC = df1.corr(df2)\n",
    "# RB = (sum(pred_list)-sum(real_list))/sum(real_list)\n",
    "# RMSE= mean_squared_error(y_true, y_pred)**0.5\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# MAE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "# 读取csv文件\n",
    "\n",
    "# df1 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\ismn_yellow_result.csv\", index_col=0)\n",
    "# df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\smap_ismnsites.csv\", index_col=0)\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\ismn_yellow_result.csv\", index_col=0)\n",
    "df2 = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\ESACCI_ismnsites.csv\", index_col=0)\n",
    "\n",
    "\n",
    "# # 将索引转换为datetime类型\n",
    "# df2.index = pd.to_datetime(df2.index, format='mixed')\n",
    "\n",
    "# # 舍去时间戳的时刻部分，只保留日期\n",
    "# df2.index = df2.index.date\n",
    "\n",
    "# # 你也可以将索引再转换为datetime类型\n",
    "# df2.index = pd.to_datetime(df2.index)\n",
    "# #print(df1.index)\n",
    "\n",
    "# df1.index = pd.to_datetime(df1.index, format='%Y/%m/%d')\n",
    "df1.index = pd.to_datetime(df1.index, format='mixed')\n",
    "df2.index = pd.to_datetime(df2.index, format='mixed')\n",
    "\n",
    "# 保证取值在相同时间索引下\n",
    "df1 = df1[df1.index.isin(df2.index)]\n",
    "df2 = df2[df2.index.isin(df1.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02034405384339311\n",
      "站点CST-01的相关系数为: 0.29514104948517106, 样本数为: -0.08547635355156759\n",
      "站点CST-02的相关系数为: 0.42420041584277846, 样本数为: -0.2780617261616702\n",
      "站点CST-03的相关系数为: 0.4649032414432493, 样本数为: -0.050444803972251664\n",
      "站点CST-04的相关系数为: 0.2685625437216271, 样本数为: -0.1122227768656989\n",
      "站点CST-05的相关系数为: 0.3132562293470273, 样本数为: -0.2164316008755573\n",
      "站点GUYUAN的相关系数为: 0.43811918770708264, 样本数为: 0.576234857013575\n",
      "站点HUANXIAN的相关系数为: 0.31519533937912886, 样本数为: 0.7731893936050442\n",
      "站点LUSHI的相关系数为: 0.3209609440626402, 样本数为: -0.11239149022334588\n",
      "站点NST-01的相关系数为: 0.451884229549578, 样本数为: -0.15036771749237904\n",
      "站点NST-02的相关系数为: 0.24563060270305734, 样本数为: -0.2560800977474874\n",
      "站点NST-03的相关系数为: 0.4082997083544696, 样本数为: -0.330437471966594\n",
      "站点NST-04的相关系数为: 0.20494019009152528, 样本数为: -0.5124499887031816\n",
      "站点NST-05的相关系数为: 0.5517830431203831, 样本数为: -0.4098644388765374\n",
      "站点NST-06的相关系数为: 0.46774097129510545, 样本数为: 0.04941111300920256\n",
      "站点NST-07的相关系数为: 0.5014728423539372, 样本数为: -0.12116205362525918\n",
      "站点NST-08的相关系数为: 0.564677069284017, 样本数为: 0.19884914518377042\n",
      "站点NST-09的相关系数为: 0.6045473365492281, 样本数为: 0.6051815481634106\n",
      "站点NST-10的相关系数为: 0.5350382029835262, 样本数为: -0.28223179269331583\n",
      "站点NST-11的相关系数为: 0.40734983300561695, 样本数为: -0.3758418647590324\n",
      "站点NST-12的相关系数为: 0.20467844812792851, 样本数为: 0.19963392175475195\n",
      "站点NST-13的相关系数为: 0.4125716469807272, 样本数为: -0.06905055331387873\n",
      "站点NST-14的相关系数为: 0.31224438218847445, 样本数为: -0.27159077730795395\n",
      "站点NST-15的相关系数为: 0.22563519298823126, 样本数为: -0.4802678015878906\n",
      "站点NST-21的相关系数为: 0.35902680668115494, 样本数为: 0.2175570764483834\n",
      "站点NST-22的相关系数为: 0.18452085098593757, 样本数为: 0.27965589402660795\n",
      "站点NST-24的相关系数为: 0.32112181642080617, 样本数为: 0.7618849115642793\n",
      "站点NST-25的相关系数为: 0.5035996970837407, 样本数为: 0.16295003523342516\n",
      "站点NST-30的相关系数为: 0.8307741605555501, 样本数为: -0.015384892425870246\n",
      "站点NST-31的相关系数为: 0.4329676007346715, 样本数为: -0.245366541283721\n",
      "站点NST-32的相关系数为: 0.17606629368488383, 样本数为: -0.2636424758951529\n",
      "站点TIANSHUI的相关系数为: 0.29449488572271854, 样本数为: 0.5900253613396005\n",
      "站点TONGWEI的相关系数为: 0.26475943839183796, 样本数为: 0.825030401990207\n",
      "站点XIFENGZH的相关系数为: 0.5607675729721562, 样本数为: 0.38131948312645497\n",
      "站点XINXIAN的相关系数为: 0.2610408421114413, 样本数为: -0.033366477260764256\n",
      "站点YONGNING的相关系数为: 0.24452688749663712, 样本数为: -0.2367475613508443\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#############################################\n",
    "# # 找出要删除的行的索引\n",
    "# rows_to_delete_df1 = df1.index[df1.isnull().any(axis=1)]\n",
    "# rows_to_delete_df2 = df2.index[df2.isnull().any(axis=1)]\n",
    "\n",
    "# # 使用 append 方法将两个索引对象拼接成一个新的索引对象\n",
    "# combined_delete_index = rows_to_delete_df1.append(rows_to_delete_df2)\n",
    "# # 将要删除的行从 df1 和 df2 中同时删除\n",
    "# df1 = df1.drop(combined_delete_index)\n",
    "# df2 = df2.drop(combined_delete_index)\n",
    "#############################################\n",
    "\n",
    "# # 打印每个时间索引下的每个站点的数值\n",
    "# for time_index in df1.index:\n",
    "#     #print(f\"时间索引：{time_index}\")\n",
    "#     for column in df1.columns:\n",
    "#         if column in df2.columns:\n",
    "#             #print(f\"站点{column}在文件1中的数值为: {df1.at[time_index, column]}，在文件2中的数值为: {df2.at[time_index, column]}\")\n",
    "#             pass\n",
    "\n",
    "# 计算相关系数\n",
    "correlations = {}\n",
    "list = []\n",
    "for column in df1.columns:\n",
    "    if column in df2.columns:\n",
    "        # 找出要删除的行的索引\n",
    "        # print(df1[column].index[df1[column].isnull().any(axis=1)])\n",
    "        # rows_to_delete_df1 = df1[column].index[df1[column].isnull().any()]\n",
    "        # rows_to_delete_df2 = df2[column].index[df2[column].isnull().any()]\n",
    "        rows_to_delete_df1 = df1[column][df1[column].isnull()].index\n",
    "        rows_to_delete_df2 = df2[column][df2[column].isnull()].index\n",
    "\n",
    "        # 使用 append 方法将两个索引对象拼接成一个新的索引对象\n",
    "        combined_delete_index = rows_to_delete_df1.append(rows_to_delete_df2)\n",
    "        # 将要删除的行从 df1 和 df2 中同时删除\n",
    "        df1[column] = df1[column].drop(combined_delete_index)\n",
    "        df2[column] = df2[column].drop(combined_delete_index)\n",
    "        #print(df1)\n",
    "        \n",
    "        if df1[column].count() == 0 or df2[column].count() == 0:\n",
    "            pass\n",
    "        else:\n",
    "            #pandas的corr自动忽略Nan值\n",
    "            #correlations数据结构为{‘站名’:[相关系数, 样本数]}\n",
    "            CC = df1[column].corr(df2[column])\n",
    "            #print(CC)\n",
    "            RB = (df2[column].sum()-df1[column].sum())/df1[column].sum()\n",
    "            list.append(RB)\n",
    "            #print(RB)\n",
    "            RMSE = 0\n",
    "            MAE = 0\n",
    "            COUNT = 0\n",
    "            # RMSE= mean_squared_error(df1[column], df2[column])**0.5\n",
    "            # MAE = mean_absolute_error(df1[column], df2[column])\n",
    "            # COUNT = df1[column].notnull().sum()\n",
    "\n",
    "            correlations[column] = [CC,RB,RMSE,MAE, COUNT]\n",
    "\n",
    "cc_csv = pd.DataFrame(columns=['站名','CC','RB','RMSE','MAE','样本数'])\n",
    "print(np.mean(list))\n",
    "# 打印相关系数\n",
    "for site, corr in correlations.items():\n",
    "    #相关系数＞0.6可以接受，样本数较少是因为smap基本只有2015年以后的数据，这样situ的样本数就会少很多,说明2015以前的situ数据可以用\n",
    "    #if corr[0] > 0.6:\n",
    "    print(f\"站点{site}的相关系数为: {corr[0]}, 样本数为: {corr[1]}\")\n",
    "    cc_csv = cc_csv._append({'站名':site,'CC':corr[0],'RB':corr[1],'RMSE':corr[2],'MAE':corr[3],'样本数':corr[4]},ignore_index=True)\n",
    "\n",
    "#cc_csv.to_csv(r\"C:\\Users\\pc\\Desktop\\嗑盐\\Y_data\\ismn_smap_yellow_cc_all.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            1.0\n",
      "1            2.0\n",
      "2            3.0\n",
      "3            4.0\n",
      "4            NaN\n",
      "5      8888888.0\n",
      "6    888888888.0\n",
      "Name: A, dtype: float64\n",
      "     A     B\n",
      "0  1.0   7.0\n",
      "1  2.0   8.0\n",
      "2  3.0   NaN\n",
      "3  4.0  10.0\n",
      "4  NaN  11.0\n",
      "   A     B\n",
      "0  1   7.0\n",
      "1  2   8.0\n",
      "2  3   9.0\n",
      "3  4  10.0\n",
      "4  5   NaN\n",
      "===================\n",
      "     A     B\n",
      "0  1.0   7.0\n",
      "1  2.0   8.0\n",
      "3  4.0  10.0\n",
      "   A     B\n",
      "0  1   7.0\n",
      "1  2   8.0\n",
      "3  4  10.0\n"
     ]
    }
   ],
   "source": [
    "# # 验证Nan值的删除操作，保证两个list的索引一致，个数一致\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # 假设你的数据已经被过滤为只包含相同时间索引的 df1 和 df2\n",
    "# # 为了演示目的，这里创建一些示例数据\n",
    "# data1 = {'A': [1, 2, 3, 4, None,8888888,888888888],\n",
    "#          'B': [7, 8, None, 10, 11,99999999,999999999]}\n",
    "# data2 = {'A': [1, 2, 3, 4, 5],\n",
    "#          'B': [7, 8, 9, 10, None]}\n",
    "\n",
    "# df1 = pd.DataFrame(data1)\n",
    "# # print(df1['A'])\n",
    "# df2 = pd.DataFrame(data2)\n",
    "\n",
    "# # 保证取值在相同时间索引下\n",
    "# df1 = df1[df1.index.isin(df2.index)]\n",
    "# df2 = df2[df2.index.isin(df1.index)]\n",
    "\n",
    "# print(df1)\n",
    "# print(df2)\n",
    "\n",
    "# print('===================')\n",
    "# # 找出要删除的行的索引\n",
    "# rows_to_delete_df1 = df1.index[df1.isnull().any(axis=1)]\n",
    "# rows_to_delete_df2 = df2.index[df2.isnull().any(axis=1)]\n",
    "\n",
    "# # 使用 append 方法将两个索引对象拼接成一个新的索引对象\n",
    "# combined_delete_index = rows_to_delete_df1.append(rows_to_delete_df2)\n",
    "# # 将要删除的行从 df1 和 df2 中同时删除\n",
    "# df1 = df1.drop(combined_delete_index)\n",
    "# df2 = df2.drop(combined_delete_index)\n",
    "\n",
    "# print(df1)\n",
    "# print(df2)\n",
    "# # 现在 df1 和 df2 中都不包含要删除的行，这两组行是在 df1 或 df2 中某项值为空的行\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s_m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
